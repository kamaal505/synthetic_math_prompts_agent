# ğŸ§  Synthetic Prompt Agent

A multi-stage LLM framework for generating **mathematically valid**, **LLM-breaking** problems. This system automates the creation, validation, and evaluation of STEM prompts, and exposes both CLI and REST API interfaces. Each problem is checked for correctness, pedagogical quality, and ability to fool a target LLM.

---

## ğŸš§ Motivation

High-quality automated prompt generation is hard. This agent focuses on:

- ğŸ” Engineering math problems with structured hints
- âœ… Validating them for correctness and clarity
- ğŸ§ª Testing whether LLMs like O3, Gemini, or DeepSeek can solve them
- ğŸ’¥ Accepting only those problems that *break* the target model

Optionally, it also performs **search-based similarity scoring** to avoid duplicates.

---

## ğŸ§± Architecture Overview

```

project-root/
â”œâ”€â”€ core/             # Core orchestration + generation logic
â”œâ”€â”€ utils/            # Shared utilities (costs, prompts, taxonomy, etc.)
â”œâ”€â”€ app/              # FastAPI backend for serving pipeline
â”œâ”€â”€ tests/            # Unit tests
â”œâ”€â”€ results/          # Output from each generation run
â”œâ”€â”€ .env              # API keys + config
â””â”€â”€ requirements.txt  # Python dependencies

````

---

## âš™ï¸ Components

### ğŸ§  `core/`

Implements the three-agent loop:

| Role       | Purpose                                   |
|------------|-------------------------------------------|
| Engineer   | Generates math problem, answer, and hints |
| Checker    | Validates content and final answer        |
| Target     | Attempts to solve the problem             |

Also includes:

- Batch orchestration
- CLI interface
- Token + cost tracking
- Search augmentation (`Tavily + GPT reranker`)

ğŸ“„ See: [`core/README.md`](core/README.md)

---

### ğŸ› ï¸ `utils/`

Shared support logic:

- âœ… Config + taxonomy loading
- ğŸ’° Token accounting and cost estimation
- ğŸ§¼ Robust JSON parsing from LLMs
- ğŸš¨ Centralized logging and error handling
- ğŸ“ Embedding + cosine similarity tools

ğŸ“„ See: [`utils/README.md`](utils/README.md)

---

### ğŸš€ `app/` (FastAPI Backend)

Exposes a REST API for:

- Starting a new generation batch
- Tracking batch status and cost
- Querying generated problems
- Managing metadata (target model, similarity, etc.)

Database-backed via SQLAlchemy.

ğŸ“„ See: [`app/README.md`](app/README.md)

---

### ğŸ§ª `tests/`

Unit tests for:

- Generation logic
- Prompt validation
- Cost tracking
- API endpoints

ğŸ“„ See: [`tests/README.md`](tests/README.md)

---

## ğŸ” API Keys

Create a `.env` file in the root directory:

```env
OPENAI_KEY=your-openai-key
GEMINI_KEY=your-gemini-key
DEEPSEEK_KEY=your-fireworks-key
TAVILY_API_KEY=your-tavily-key
DATABASE_URL=sqlite:///./database/math_agent.db
````

---

## ğŸš€ Getting Started

### 1. Install dependencies

```bash
pip install -r requirements.txt
```

---

### 2. Run CLI (Core Pipeline)

Run interactively:

```bash
python core/cli/run_interactive.py
```

Or use a YAML config:

```bash
python core/cli/interface.py --config configs/sample_config.yaml
```

Results saved to:

```
results/
â””â”€â”€ run_2025_07_03_14_15_00/
    â”œâ”€â”€ valid.json
    â”œâ”€â”€ discarded.json
    â””â”€â”€ costs.json
```

---

### 3. Run FastAPI Backend

```bash
uvicorn app.main:app --reload
```

Now visit: [http://localhost:8000/api/docs](http://localhost:8000/api/docs)

You can:

* POST to `/api/generation/` to start batch generation
* GET `/api/generation/status/{batch_id}` to track progress
* Query prompts via `/api/problems/` or `/api/batches/`

---

## ğŸ“¦ Example: Start a Batch via API

```bash
curl -X POST http://localhost:8000/api/generation \
  -H "Content-Type: application/json" \
  -d '{
    "num_problems": 5,
    "engineer_model": {"provider": "gemini", "model_name": "gemini-2.5-pro"},
    "checker_model": {"provider": "openai", "model_name": "o3"},
    "target_model": {"provider": "openai", "model_name": "o3"},
    "taxonomy": { "Algebra": ["Quadratics"] },
    "use_search": true
  }'
```

---

## ğŸ“ˆ Output Structure

Each accepted prompt includes:

* âœ… Validated `problem`, `answer`, `hints`
* ğŸ’¥ Target model output
* ğŸ“ Similar problems (via Tavily + GPT)
* ğŸ’° Cost + token usage
* ğŸ§  Embedding vector (for deduplication)

---

## ğŸ§  Acceptance Criteria

A problem is accepted if:

* It passes all validation checks
* The target model fails to solve it
* (Optional) Similarity is below threshold to known problems

---

## ğŸ’¬ Status Endpoints

* `/api/` â†’ Welcome message
* `/api/health` â†’ Health check
* `/api/generation/status/{batch_id}` â†’ Generation progress
* `/api/problems/` â†’ Query prompts with filters
* `/api/batches/` â†’ View batch summary + statistics

---

## ğŸ§ª Running Tests

```bash
pytest tests/
```

---

## ğŸ“„ License

MIT License

---

## ğŸ™Œ Credits

Mirza Kamaal
Tom Mathews
Mintesnote Bankisra

```