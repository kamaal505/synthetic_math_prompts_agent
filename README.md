Here is a comprehensive README for your **Synthetic Prompt Agent** project. It captures the full pipeline, modular architecture, API usage, and key developer notes:

---

# ğŸ§  Synthetic Prompt Agent

A modular framework for generating, validating, and filtering high-quality synthetic math prompts to stress-test LLMs. This system orchestrates multiple LLMs (e.g., OpenAI, Gemini, DeepSeek) to produce mastery-level questions with hints, verify their correctness, and assess whether a target model fails to solve them correctly.

---

## ğŸš€ Key Features

* **Multi-LLM Orchestration**: Separate roles for *engineer*, *checker*, and *target* models.
* **Strict Output Validation**: Ensures correctness and format adherence via semantic and structural checks.
* **Model Breaking Logic**: Accepts only those valid problems that the target model answers incorrectly.
* **Hint Generation and Repair**: Automatically retries if hint generation fails.
* **Cost Tracking**: Tracks and logs token usage per model.
* **FastAPI Backend**: Production-ready API with strict Pydantic schemas.
* **CLI Support**: Interactive testing and batch generation via terminal.
* **Modular Architecture**: Clean separation of LLM logic, pipeline orchestration, and utilities.

---

## ğŸ—‚ Project Structure

```
synthetic_prompt_agent/
â”œâ”€â”€ core/                  # Core logic for prompt generation, validation, evaluation
â”‚   â”œâ”€â”€ generate_prompt.py
â”‚   â”œâ”€â”€ validate_prompt.py
â”‚   â”œâ”€â”€ evaluate_target_model.py
â”‚   â”œâ”€â”€ llm/               # Model-specific dispatchers and cost trackers
â”‚   â”‚   â”œâ”€â”€ llm_dispatch.py
â”‚   â”‚   â”œâ”€â”€ openai_utils.py
â”‚   â”‚   â””â”€â”€ ...
â”œâ”€â”€ utils/                 # Utility functions
â”‚   â”œâ”€â”€ taxonomy.py
â”‚   â”œâ”€â”€ cost_tracker.py
â”‚   â”œâ”€â”€ validation.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ app/                   # FastAPI backend
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ routes.py
â”‚   â”œâ”€â”€ schemas.py
â”‚   â””â”€â”€ pipeline_service.py
â”œâ”€â”€ cli/                   # CLI interface
â”‚   â”œâ”€â”€ run_interactive.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ save_results.py        # Handles saving valid, discarded, and cost logs
â”œâ”€â”€ generate_batch.py      # Main entrypoint for generating a batch of prompts
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md              # â† You're here
â””â”€â”€ ...
```

---

## âš™ï¸ How It Works

1. **Input**: A configuration specifying:

   * Desired number of prompts
   * Model configurations for engineer, checker, and target
   * Optional `taxonomy` (subject/topic) for sampling

2. **Engineer Role**:

   * Generates problem, answer, and list of hints
   * Retries if hints are empty

3. **Validation Stage**:

   * Checks format, correctness, and mathematical validity
   * Ensures `hints` is a non-empty `List[str]`

4. **Target Model Evaluation**:

   * Model attempts the problem
   * Answer is compared semantically with ground truth

5. **Final Filtering**:

   * Only prompts that are **valid** and **break the target model** are saved as `valid.json`
   * Others go to `discarded.json`

6. **Cost Logging**:

   * Token usage is logged per model and saved to `costs.json`

---

## ğŸ§ª Run a Batch from CLI

```bash
python generate_batch.py \
    --num_prompts 10 \
    --engineer_model '{"provider": "gemini", "model": "gemini-2.5-pro-preview-03-25"}' \
    --checker_model '{"provider": "deepseek", "model": "deepseek-reasoner"}' \
    --target_model '{"provider": "openai", "model": "o3"}' \
    --taxonomy taxonomy/math_taxonomy.json
```

---

## ğŸŒ API Usage (FastAPI)

### ğŸ”— POST `/generate`

Generate a batch of prompts using your configuration.

#### Request

```json
{
  "num_prompts": 5,
  "engineer_model": {
    "provider": "gemini",
    "model": "gemini-2.5-pro-preview-03-25"
  },
  "checker_model": {
    "provider": "deepseek",
    "model": "deepseek-reasoner"
  },
  "target_model": {
    "provider": "openai",
    "model": "o3"
  },
  "taxonomy": {
    "Algebra": ["Polynomials", "Inequalities"]
  }
}
```

#### Response

```json
{
  "run_id": "20250626_183721",
  "num_accepted": 4,
  "num_attempted": 5,
  "valid_problems": [ ... ],
  "discarded_problems": [ ... ]
}
```

---

## ğŸ§¾ Output Files

Each run produces:

* `valid.json`: Fully validated, target-broken prompts
* `discarded.json`: Invalid or solved by target
* `costs.json`: Token usage and cost data (not exposed via API)

Files are saved to `./output/<run_id>/`.

---

## ğŸ§  Model Configuration

Each model is passed explicitly as:

```json
{
  "provider": "openai" | "gemini" | "deepseek",
  "model": "o3" | "gemini-2.5-pro-preview-03-25" | ...
}
```

This design enforces transparency and allows local testing before deployment.

---

## ğŸ§° Developer Notes

* Add new models by updating `llm_dispatch.py` and respective utility modules.
* All token tracking is centralized via `CostTracker`.
* Run-level logic should go through `generate_batch.py` or `pipeline_service.py`.

---

## âœ… TODO / Roadmap

* [ ] Database logging & search
* [ ] `/progress/{run_id}` endpoint
* [ ] Frontend dashboard (Django or React)
* [ ] Test suite for all core modules
* [ ] Web search & Google proof augmentation

---

##  Contributors

* Project Lead: **You**
* Architected with: OpenAI, Gemini, DeepSeek APIs

---