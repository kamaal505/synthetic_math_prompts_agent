# ğŸš€ FastAPI Backend â€“ Synthetic Prompt Agent

This module exposes a REST API to orchestrate, track, and manage synthetic math prompt generation. It supports:

- Launching LLM-based generation runs
- Tracking batch progress
- Querying all generated prompts (valid, discarded, solved)
- Updating batch metadata and target model
- Storing metadata like cost, similarity, embeddings

---

## ğŸ“ Directory Structure

```

app/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ routes.py              # Main API router
â”‚   â”œâ”€â”€ batches.py             # /batches â€“ batch management endpoints
â”‚   â”œâ”€â”€ generation.py          # /generation â€“ start/track generation jobs
â”‚   â””â”€â”€ problems.py            # /problems â€“ prompt-level access
â”œâ”€â”€ config.py                  # App settings and validation
â”œâ”€â”€ main.py                    # FastAPI app entrypoint
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ database.py            # SQLAlchemy engine + session
â”‚   â”œâ”€â”€ models.py              # ORM models: Batch, Problem
â”‚   â””â”€â”€ schemas.py             # Pydantic schemas
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ batch\_service.py       # Batch-related DB logic
â”‚   â”œâ”€â”€ problem\_service.py     # Problem-related DB logic
â”‚   â””â”€â”€ pipeline\_service.py    # Launches and tracks core pipeline

````

---

## ğŸ”Œ Key Features

### âœ… Generation (via POST `/api/generation/`)

Starts a generation run with background execution:

```json
{
  "num_problems": 10,
  "engineer_model": {"provider": "gemini", "model_name": "gemini-2.5-pro"},
  "checker_model": {"provider": "openai", "model_name": "o3"},
  "target_model": {"provider": "openai", "model_name": "o3"},
  "taxonomy": { "Algebra": ["Quadratics"] },
  "use_search": true
}
````

* Launches `core.runner.run_pipeline_from_config()` as a background task
* Persists each valid/discarded prompt to the DB
* Saves embeddings, similarity matches, and batch cost

---

### ğŸ“¦ Batches API â€“ `/api/batches`

Manage generation batches and metadata.

* `GET /batches/` â€“ List all batches with prompt stats
* `GET /batches/{batch_id}` â€“ Fetch a specific batch + stats
* `PATCH /batches/{batch_id}/target-model` â€“ Update target model
* `DELETE /batches/{batch_id}` â€“ Delete a batch
* `GET /batches/problems/count` â€“ Get total prompt count or per-batch

---

### ğŸ§  Problems API â€“ `/api/problems`

Access and filter problems across all batches.

* `GET /problems/` â€“ Paginated query with filters (`batch_id`, `status`)
* `GET /problems/problem/{id}` â€“ Fetch single problem
* `GET /problems/batch/{batch_id}/problems` â€“ All problems for a batch
* `GET /problems/all` â€“ All problems (unlimited; use with caution)

Each problem includes:

* Rejection reason
* Target model answer
* Embedding vector
* Similarity matches
* Cost and status (`valid`, `discarded`, etc.)

---

### ğŸ“Š Generation Status â€“ `/api/generation/status/{batch_id}`

Returns live progress of a running or completed batch:

```json
{
  "batch_id": 12,
  "total_needed": 10,
  "valid_generated": 8,
  "total_generated": 14,
  "progress_percentage": 80.0,
  "stats": {
    "valid": 8,
    "discarded": 6
  },
  "batch_cost": 0.0952,
  "status": "in_progress"
}
```

---

## ğŸ§¾ Schemas

Located in `app/models/schemas.py`. Includes:

* `GenerationRequest` / `GenerationResponse`
* `Batch`, `BatchWithStats`
* `Problem`, `ProblemResponse`
* `PipelineConfig`, `ModelConfig`
* `GenerationStatus`

---

## ğŸ§± Database Models

### `Batch`

* Taxonomy JSON
* Pipeline config (engineer, checker, target)
* Number of problems
* Batch-level cost
* Created/updated timestamps

### `Problem`

* Subject, topic, problem, answer, hints
* Rejection reason
* Status: `valid`, `discarded`, or `solved`
* Embeddings
* Similar problems
* Token-level cost
* Target model output

---

## âš™ï¸ Configuration â€“ `config.py`

Reads `.env` file and exposes global `settings` object:

```dotenv
OPENAI_KEY=...
GEMINI_KEY=...
DEEPSEEK_KEY=...
DATABASE_URL=sqlite:///./database/math_agent.db
```

Also sets:

* `SIMILARITY_THRESHOLD = 0.82`
* `EMBEDDING_MODEL = "text-embedding-3-small"`

---

## ğŸ©º Health Check

```
GET / â†’ { "message": "Synthetic Math Prompts API" }
GET /health â†’ { "status": "healthy" }
```

---

## ğŸš€ Running Locally

1. Install dependencies:

```bash
pip install -r requirements.txt
```

2. Create `.env` file with API keys

3. Run the server:

```bash
uvicorn app.main:app --reload
```

The API will be available at: [http://localhost:8000/api](http://localhost:8000/api)

---

## ğŸ§ª Example Query: List all valid problems from a batch

```http
GET /api/problems?batch_id=12&status=valid
```

---

## ğŸ§ª Example: Start a new batch

```bash
curl -X POST http://localhost:8000/api/generation \
  -H "Content-Type: application/json" \
  -d '{
    "num_problems": 5,
    "engineer_model": { "provider": "gemini", "model_name": "gemini-2.5-pro" },
    "checker_model": { "provider": "openai", "model_name": "o3" },
    "target_model": { "provider": "openai", "model_name": "o3" },
    "taxonomy": { "Algebra": ["Quadratics"] },
    "use_search": true
  }'
```

---