# ğŸ§  Core Logic â€“ Synthetic Prompt Generation Engine

This module implements the core logic of a multi-stage agent pipeline that produces **mathematically valid**, **LLM-breaking** math problems. It features three distinct LLM roles:

- **Engineer**: Generates math problems, answers, and hints
- **Checker**: Validates problem structure and evaluates model answers
- **Target Model**: Attempts to solve the problem

Only problems that are both **valid** and **unsolved by the target model** are accepted. If enabled, the system also augments accepted problems with **similarity metadata** using web search and GPT reranking.

---

## ğŸ“ Directory Structure

```

core/
â”œâ”€â”€ checker/
â”‚   â””â”€â”€ validate\_prompt.py            # Validates problem and target model answer
â”œâ”€â”€ engineer/
â”‚   â””â”€â”€ generate\_prompt.py            # Generates problems, answers, and hints
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ llm\_dispatch.py               # Role-based dispatcher
â”‚   â””â”€â”€ openai\_utils.py               # Token-aware OpenAI wrapper
â”œâ”€â”€ orchestration/
â”‚   â”œâ”€â”€ generate\_batch.py             # Batch generation loop
â”‚   â””â”€â”€ evaluate\_target\_model.py      # Target model solver
â”œâ”€â”€ search/
â”‚   â”œâ”€â”€ agent.py                      # GPT reranker over Tavily results
â”‚   â”œâ”€â”€ tavily\_search.py              # Tavily API search interface
â”‚   â”œâ”€â”€ search\_similarity.py          # Similarity scoring orchestrator
â”‚   â””â”€â”€ **init**.py                   # Exports score\_similarity()
â”œâ”€â”€ cli/
â”‚   â”œâ”€â”€ interface.py                  # Batch mode CLI
â”‚   â””â”€â”€ run\_interactive.py            # Interactive experimentation
â”œâ”€â”€ runner.py                         # Entrypoint for backend or service integration

````

---

## ğŸ§  Engineering: `engineer/generate_prompt.py`

```python
generate_full_problem(seed, subject, topic, provider, model_name) â†’ dict
````

* Generates `problem`, `answer`, `hints` (as a list of strings)
* Uses structured prompts (from `system_messages.py`)
* Validates output format
* Returns:

  * `problem`, `answer`, `hints`
  * `tokens_prompt`, `tokens_completion`, `raw_output`, `raw_prompt`

---

## âœ… Validation: `checker/validate_prompt.py`

```python
validate_problem(problem_data, mode, provider, model_name) â†’ dict
```

Modes:

* `"initial"`: checks formatting, structure, and hint validity
* `"equivalence_check"`: compares target model output to correct answer

Returns:

```json
{
  "valid": true,
  "reason": "...",
  "corrected_hints": [...],
  "tokens_prompt": 123,
  "tokens_completion": 456
}
```

---

## ğŸ¤– Evaluation: `orchestration/evaluate_target_model.py`

```python
model_attempts_answer(problem: str, model_config: dict) â†’ dict
```

* Sends the problem to a target LLM
* Returns:

  * `output` (target model answer)
  * `tokens_prompt`, `tokens_completion`

---

## ğŸ”€ Role Dispatcher: `llm/llm_dispatch.py`

Centralizes model-specific logic and routes all LLM calls.

```python
call_engineer(...)
call_checker(..., mode)
call_target_model(...)
```

All calls return output + token usage.

---

## ğŸŒ€ Batch Loop: `orchestration/generate_batch.py`

```python
run_generation_pipeline(config: dict) â†’ (valid, discarded, cost_tracker)
```

Pipeline:

1. Sample subject/topic from taxonomy (or override)
2. (Optional) Run web search for seed prompt
3. Engineer generates a problem
4. Checker validates it
5. Target model attempts it
6. Checker compares answers
7. If accepted and `use_search=True`, similarity metadata is added

Tracks:

* Accepted vs discarded prompts
* Per-model token usage
* Cost per provider
* Search metadata

---

## ğŸ” Search Similarity (Optional)

Located in `core/search/`. Adds relevance metadata to accepted prompts by comparing them to real math forum questions.

### ğŸ”§ Key Components

* `tavily_search.py`: Sends queries to Tavily with `include_answer=True`
* `agent.py`: Uses GPT to score similarity between generated problems and Tavily results
* `search_similarity.py`: Orchestrates search + reranking
* `__init__.py`: Exports `score_similarity()`

### â• Output Fields (if enabled)

Each valid prompt includes:

```json
{
  "similarity_score": 0.82,
  "top_matches": [
    {"title": "...", "content": "..."},
    ...
  ]
}
```

---

## ğŸ’° Cost Tracking

Handled via `utils/costs.py`, with full token and price tracking:

* âœ… OpenAI (chat + responses)
* âœ… Gemini (estimated from char count)
* âœ… DeepSeek

Stored in `.costs.json` per run.

---

## ğŸ§ª CLI Entry Points

* `interface.py`: Run from YAML config
* `run_interactive.py`: Manual entry for experimentation

All results saved to:

```
results/<run_id>/
```

---

## âœ… Acceptance Criteria

A prompt is **accepted** only if:

* It passes checker validation
* The target model answers incorrectly (i.e., is "broken")
* \[Optional] Search metadata can be added afterward

---

## ğŸ“‚ Output Files

```
results/
â””â”€â”€ run_2025_07_03_14_15_00/
    â”œâ”€â”€ valid.json         # Accepted + validated + model-breaking
    â”œâ”€â”€ discarded.json     # Invalid or target-model-solved
    â””â”€â”€ costs.json         # Token usage + pricing metadata
```

---

## ğŸ” Environment Variables

Use `.env` at the project root:

```dotenv
OPENAI_KEY=...
GEMINI_KEY=...
DEEPSEEK_KEY=...
TAVILY_API_KEY=...
```

```