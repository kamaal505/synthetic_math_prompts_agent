import concurrent.futures
import threading
from random import choice
from typing import Any, Dict, List, Tuple

from core.llm.llm_dispatch import call_checker, call_engineer, call_target_model
from core.orchestration.concurrent_processor import create_concurrent_processor
from utils.cost_estimation import safe_log_cost
from utils.costs import CostTracker
from utils.curriculum_strategy import create_curriculum_strategy
from utils.logging_config import get_logger
from utils.performance_monitor import get_performance_monitor
from utils.validation import assert_valid_model_config

# Get logger instance
logger = get_logger(__name__)


def _generate_batch_problems(
    config: Dict[str, Any], cost_tracker: CostTracker, batch_size: int = 3
) -> Tuple[str, Dict[str, Any]]:
    """
    Generate a batch of problems using concurrent processing for improved efficiency.

    Args:
        config: Configuration dictionary
        cost_tracker: CostTracker instance
        batch_size: Number of problems to generate concurrently

    Returns:
        Tuple of (result_type, data_dict) containing multiple problems
    """
    thread_id = threading.current_thread().ident
    logger.info(f"üßµ Thread {thread_id} generating batch of {batch_size} problems concurrently")

    taxonomy = config.get("taxonomy")
    engineer_cfg = config["engineer_model"]

    # Initialize curriculum strategy for intelligent topic selection
    curriculum_strategy = create_curriculum_strategy(taxonomy) if taxonomy else None

    try:
        # Use ThreadPoolExecutor for concurrent problem generation
        with concurrent.futures.ThreadPoolExecutor(max_workers=min(batch_size, 5)) as executor:
            # Submit all generation tasks concurrently
            future_to_index = {}
            for i in range(batch_size):
                # Use curriculum strategy for topic selection if available
                if curriculum_strategy:
                    subject, topic, difficulty_level, topic_description = curriculum_strategy.select_topic_and_difficulty()
                else:
                    # Fallback to legacy random selection
                    subject = choice(list(taxonomy.keys())) if taxonomy else config.get("subject")
                    topic = choice(taxonomy[subject]) if taxonomy else config.get("topic")
                    difficulty_level = None
                
                future = executor.submit(call_engineer, subject, topic, None, engineer_cfg, difficulty_level)
                future_to_index[future] = i

            problems = []
            # Collect results as they complete
            for future in concurrent.futures.as_completed(future_to_index):
                batch_index = future_to_index[future]
                try:
                    engineer_result = future.result()
                    safe_log_cost(
                        cost_tracker,
                        engineer_cfg,
                        engineer_result.get("tokens_prompt", 0),
                        engineer_result.get("tokens_completion", 0),
                        raw_output=engineer_result.get("raw_output", ""),
                        raw_prompt=engineer_result.get("raw_prompt", ""),
                    )

                    problem_data = {
                        "subject": subject,
                        "topic": topic,
                        "problem": engineer_result["problem"],
                        "answer": engineer_result["answer"],
                        "hints": engineer_result["hints"],
                        "batch_index": batch_index,
                    }
                    problems.append(problem_data)
                    logger.debug(f"‚úÖ Completed batch problem {batch_index + 1}/{batch_size}")

                except Exception as e:
                    logger.error(f"üö® Error generating problem {batch_index}: {str(e)}")
                    # Continue with other problems even if one fails

        if not problems:
            return "error", {
                "error": "No problems generated successfully",
                "subject": subject,
                "topic": topic,
                "batch_size": batch_size,
            }

        logger.info(f"üéØ Successfully generated {len(problems)}/{batch_size} problems in batch")
        return "batch_generated", {"problems": problems, "batch_size": len(problems)}

    except Exception as e:
        logger.error("üö® Error in batch generation: %s", str(e), exc_info=True)
        return "error", {
            "error": str(e),
            "subject": subject,
            "topic": topic,
            "batch_size": batch_size,
        }


def _generate_and_validate_prompt(
    config: Dict[str, Any], cost_tracker: CostTracker
) -> Tuple[str, Dict[str, Any]]:
    """
    Generate and validate a single prompt with enhanced error handling and performance monitoring.

    Args:
        config: Configuration dictionary containing model configs and generation parameters
        cost_tracker: CostTracker instance for logging costs

    Returns:
        tuple: (result_type, data_dict) where result_type is one of:
                - "accepted": prompt was generated and target model failed
                - "discarded": prompt was rejected or target model succeeded
                - "error": an error occurred during generation
    """

    thread_id = threading.current_thread().ident
    logger.info(f"üßµ Thread {thread_id} starting task processing")
    
    # Start performance monitoring
    perf_monitor = get_performance_monitor()
    start_time = perf_monitor.start_request(thread_id)
    
    success = False
    cached = False
    retries = 0

    taxonomy = config.get("taxonomy")
    engineer_cfg = config["engineer_model"]
    checker_cfg = config["checker_model"]
    target_cfg = config["target_model"]

    # Initialize curriculum strategy for intelligent topic selection
    curriculum_strategy = create_curriculum_strategy(taxonomy) if taxonomy else None
    
    # Use curriculum strategy for topic selection if available
    if curriculum_strategy:
        subject, topic, difficulty_level, topic_description = curriculum_strategy.select_topic_and_difficulty()
    else:
        # Fallback to legacy random selection
        subject = choice(list(taxonomy.keys())) if taxonomy else config.get("subject")
        topic = choice(taxonomy[subject]) if taxonomy else config.get("topic")
        difficulty_level = None
        topic_description = None

    try:
        # Step 1: Engineer - with pre-filtering heuristics
        if config.get("enable_prefiltering", False):
            # Simple heuristic check before expensive API calls
            if _is_problem_too_easy(subject, topic):
                logger.info("‚ö° Pre-filtered as too easy, skipping")
                return "discarded", {
                    "subject": subject,
                    "topic": topic,
                    "difficulty_level": difficulty_level,
                    "rejection_reason": "Pre-filtered as too easy",
                }

        engineer_result = call_engineer(subject, topic, None, engineer_cfg, difficulty_level)
        safe_log_cost(
            cost_tracker,
            engineer_cfg,
            engineer_result.get("tokens_prompt", 0),
            engineer_result.get("tokens_completion", 0),
            raw_output=engineer_result.get("raw_output", ""),
            raw_prompt=engineer_result.get("raw_prompt", ""),
        )
        core = {
            "subject": subject,
            "topic": topic,
            "problem": engineer_result["problem"],
            "answer": engineer_result["answer"],
            "hints": engineer_result["hints"],
        }

        # Step 2: Checker validation
        checker_result = call_checker(core, checker_cfg, mode="initial")
        safe_log_cost(
            cost_tracker,
            checker_cfg,
            checker_result.get("tokens_prompt", 0),
            checker_result.get("tokens_completion", 0),
            raw_output=checker_result.get("raw_output", ""),
            raw_prompt=checker_result.get("raw_prompt", ""),
        )

        corrected_hints = checker_result.get("corrected_hints")

        core["hints_were_corrected"] = (
            bool(corrected_hints)
            and isinstance(corrected_hints, dict)
            and any(h.strip() for h in corrected_hints.values())
        )

        if not checker_result["valid"]:
            logger.info(f"‚ùå Rejected: {checker_result.get('reason', '')}")
            return "discarded", {
                **core,
                "rejection_reason": checker_result.get("reason", ""),
            }

        if corrected_hints:
            logger.info(f"‚úçÔ∏è Checker revised {len(corrected_hints)} hint(s).")
            core["hints"] = corrected_hints
        else:
            logger.info("‚úÖ Keeping original hints from generator.")

        # Step 3: Target model attempts (with deterministic settings)
        target_result = call_target_model(core["problem"], target_cfg)
        safe_log_cost(
            cost_tracker,
            target_cfg,
            target_result.get("tokens_prompt", 0),
            target_result.get("tokens_completion", 0),
            raw_output=target_result.get("raw_output", ""),
            raw_prompt=target_result.get("raw_prompt", ""),
        )
        core["target_model_answer"] = target_result["output"]

        # Step 4: Checker judges model's answer
        final_check = call_checker(core, checker_cfg, mode="equivalence_check")
        safe_log_cost(
            cost_tracker,
            checker_cfg,
            final_check.get("tokens_prompt", 0),
            final_check.get("tokens_completion", 0),
            raw_output=final_check.get("raw_output", ""),
            raw_prompt=final_check.get("raw_prompt", ""),
        )

        if not final_check.get("valid", False):
            logger.info("üß† Target model failed ‚Äî Accepted!")

            # Step 5: Similarity Scoring (optional, with optimization)
            if config.get("use_search", False):
                try:
                    from core.search import score_similarity

                    similarity = score_similarity(core["problem"])
                    core["similar_problems"] = {
                        "similarity_score": similarity.get("similarity_score"),
                        "top_matches": similarity.get("top_matches", []),
                    }
                    safe_log_cost(
                        cost_tracker,
                        {"provider": "openai", "model_name": "gpt-4.1"},
                        similarity.get("tokens_prompt", 0),
                        similarity.get("tokens_completion", 0),
                        raw_output=similarity.get("raw_output", ""),
                    )
                    logger.info(
                        f"üîç Similarity score: {core['similar_problems']['similarity_score']:.3f}"
                    )
                except Exception as e:
                    logger.error(
                        "‚ö†Ô∏è Error scoring similarity: %s", str(e), exc_info=True
                    )
                    core["similar_problems"] = {
                        "similarity_score": None,
                        "top_matches": [],
                    }

            success = True
            result = "accepted", core
        else:
            logger.info("üü° Model answered correctly ‚Äî Discarded.")
            success = True  # Task completed successfully, even if discarded
            result = "discarded", {
                **core,
                "rejection_reason": "Target model solved correctly",
            }

    except Exception as e:
        logger.error("üö® Error in pipeline task: %s", str(e), exc_info=True)
        success = False
        result = "error", {
            "error": str(e),
            "subject": subject,
            "topic": topic,
        }
    
    finally:
        # Record performance metrics
        perf_monitor.end_request(
            start_time=start_time,
            success=success,
            cached=cached,
            retries=retries,
            thread_id=thread_id
        )
    
    return result


def _is_problem_too_easy(subject: str, topic: str) -> bool:
    """
    Pre-filtering heuristic to identify potentially easy problems.

    Args:
        subject: Math subject
        topic: Topic within subject

    Returns:
        True if problem is likely too easy
    """
    # Simple heuristics - can be expanded
    easy_topics = {
        "arithmetic": ["addition", "subtraction", "basic multiplication"],
        "algebra": ["linear equations with one variable"],
        "geometry": ["area of rectangle", "perimeter of square"],
    }

    subject_lower = subject.lower()
    topic_lower = topic.lower()

    for easy_subject, easy_topic_list in easy_topics.items():
        if easy_subject in subject_lower:
            for easy_topic in easy_topic_list:
                if easy_topic in topic_lower:
                    return True

    return False


def run_generation_pipeline(
    config: Dict[str, Any],
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], CostTracker]:
    """
    Run the enhanced generation pipeline with adaptive threading, caching, and performance monitoring.

    Args:
        config: Configuration dictionary including num_problems, model configs, etc.

    Returns:
        tuple: (accepted_list, discarded_list, cost_tracker)
    """
    logger.info("üöÄ Starting enhanced generation pipeline with adaptive threading and caching")

    # Initialize performance monitoring
    perf_monitor = get_performance_monitor()
    perf_monitor.reset()  # Start fresh for this pipeline run

    # Validate model configurations
    engineer_cfg = config["engineer_model"]
    checker_cfg = config["checker_model"]
    target_cfg = config["target_model"]

    assert_valid_model_config("engineer", engineer_cfg)
    assert_valid_model_config("checker", checker_cfg)
    assert_valid_model_config("target", target_cfg)

    # Initialize cost tracker
    cost_tracker = CostTracker()

    # Check if we should use the new concurrent processor
    use_enhanced_processing = config.get("use_enhanced_concurrent_processing", True)

    try:
        if use_enhanced_processing:
            result = _run_enhanced_pipeline(config, cost_tracker)
        else:
            result = _run_legacy_pipeline(config, cost_tracker)
        
        # Log performance summary
        perf_monitor.log_summary()
        
        return result
        
    except Exception as e:
        logger.error("üö® Pipeline execution failed: %s", str(e), exc_info=True)
        perf_monitor.log_summary()  # Log performance even on failure
        raise


def _run_enhanced_pipeline(
    config: Dict[str, Any], cost_tracker: CostTracker
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], CostTracker]:
    """
    Run the pipeline using the enhanced concurrent processor.

    Args:
        config: Configuration dictionary
        cost_tracker: Cost tracking instance

    Returns:
        Tuple of (accepted, discarded, cost_tracker)
    """
    logger.info("Using enhanced concurrent processing with adaptive threading")

    # Create concurrent processor
    processor = create_concurrent_processor(config)

    # Define task arguments generator
    def generate_task_args():
        return (config, cost_tracker)

    # Define progress callback
    def progress_callback(progress_info: Dict[str, Any]):
        stats = progress_info.get("stats", {})
        logger.info(
            f"üìä Progress: {progress_info['approved']}/{progress_info['target']} "
            f"(Workers: {stats.get('current_workers', 'N/A')}, "
            f"Success Rate: {stats.get('success_rate', 0):.1%})"
        )

    # Process the batch
    accepted, discarded, errors = processor.process_batch(
        task_func=_generate_and_validate_prompt,
        task_args_generator=generate_task_args,
        progress_callback=progress_callback,
    )

    # Combine discarded and errors for backward compatibility
    all_discarded = discarded + errors

    logger.info(
        f"‚úÖ Enhanced pipeline completed: {len(accepted)} accepted, "
        f"{len(all_discarded)} discarded/errors"
    )

    return accepted, all_discarded, cost_tracker


def _run_legacy_pipeline(
    config: Dict[str, Any], cost_tracker: CostTracker
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], CostTracker]:
    """
    Run the pipeline using the legacy concurrent processing (for backward compatibility).

    Args:
        config: Configuration dictionary
        cost_tracker: Cost tracking instance

    Returns:
        Tuple of (accepted, discarded, cost_tracker)
    """
    logger.info("Using legacy concurrent processing")

    accepted = []
    discarded = []

    target_total = config["num_problems"]
    approved_count = 0
    attempt_counter = 0

    max_workers = config.get("max_workers", 10)
    logger.info(f"üöÄ Starting parallel execution with {max_workers} worker threads")

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []

        while approved_count < target_total:
            batch_size = min(
                max_workers * 2, target_total - approved_count + len(discarded)
            )

            while len(futures) < batch_size and (approved_count < target_total):
                attempt_counter += 1
                future = executor.submit(
                    _generate_and_validate_prompt, config, cost_tracker
                )
                futures.append((future, attempt_counter))

            if futures:
                completed_futures = []
                for future, attempt_num in futures:
                    if future.done():
                        completed_futures.append((future, attempt_num))

                if not completed_futures:
                    done_futures = concurrent.futures.as_completed(
                        [f[0] for f in futures], timeout=None
                    )
                    first_done = next(done_futures)
                    for future, attempt_num in futures:
                        if future == first_done:
                            completed_futures.append((future, attempt_num))
                            break

                for future, attempt_num in completed_futures:
                    try:
                        result_type, data = future.result()
                        logger.info(
                            f"üîß Attempt {attempt_num} ‚Äî Approved so far: {approved_count}/{target_total}"
                        )

                        if result_type == "accepted":
                            accepted.append(data)
                            approved_count += 1
                        else:
                            discarded.append(data)
                    except Exception as e:
                        logger.error(
                            "üö® Future execution error in attempt %s: %s",
                            attempt_num,
                            str(e),
                            exc_info=True,
                        )
                        discarded.append(
                            {"error": str(e), "attempt_number": attempt_num}
                        )

                    futures.remove((future, attempt_num))

    return accepted, discarded, cost_tracker
